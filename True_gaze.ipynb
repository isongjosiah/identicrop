{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "True_gaze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isongjosiah/identicrop/blob/main/True_gaze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "X2mm9FdDRz8W"
      },
      "source": [
        "# Curbing crop loss in Nigeria's Agricultural Ecosystem\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdX_wBhbS4R8"
      },
      "source": [
        "About 690 million people are hungry today and three billion cannot afford a healthy diet [[1]](https://guardian.ng/apo-press-releases/food-loss-and-waste-must-be-reduced-for-greater-food-security-and-environmental-sustainability/) yet food continues to be wasted and lost. The status of global food security is alarming, and protection against losses caused by crop pests, plant diseases, in particular, can play a critical role in improving food security worldwide and meeting the growing demand for food quality and quantity. \n",
        "\n",
        "In this project, we process diseased and healthy apple crop images and build machine learning models that can help in the classification of diseased and healthy apple crops with plans to extend this implementation to all the crops in the Nigerian agricultural ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iKNHP96UlR4"
      },
      "source": [
        "#### Data and Goals\n",
        "We made use of the [PlantVillage dataset](https://www.kaggle.com/abdallahalidev/plantvillage-dataset)  dataset which at the time of this project contained 54,343 colored images of diseased and healthy images of the following crops:\n",
        "\n",
        "\n",
        "*   Apple\n",
        "*   Corn\n",
        "*   Grape\n",
        "*   Orange\n",
        "*   Peach\n",
        "*   Pepper\n",
        "*   Potato\n",
        "*   Raspberry\n",
        "*   Soybean\n",
        "*   Squash\n",
        "*   Strawberry\n",
        "*   Tomato\n",
        "\n",
        "However, for this project we trained our models on Apple crops only due to computation contraint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-CdDEQsWgei"
      },
      "source": [
        "#### Table of Content\n",
        "\n",
        "\n",
        "*   [Data Exploration](#data-exploration)\n",
        "*   [Data Preparation](#data-preparation)\n",
        "*   [Train Validation Test Split](#train-validation-test-split)\n",
        "*   [Data Augmentation](#data-augmentation)\n",
        "*   [Feature Extraction](#feature-extraction)\n",
        "*   [Supervised Learning](#supervised-learning)\n",
        "*   [Results](#results)\n",
        "*   [Exploration for future works](#exploration-for-future-works)\n",
        "*   [Conclusions](#conclusions)\n",
        "*   [Acknowledgements](#acknowledgements)\n",
        "*   [References](#references)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NYB9Vh9Rz8n"
      },
      "source": [
        "# utilities\n",
        "from skimage import util\n",
        "import joblib\n",
        "import os\n",
        "import random\n",
        "\n",
        "# data preprocessing and machine learning\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from skimage.transform import resize, rotate\n",
        "from skimage import exposure\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy import ndimage\n",
        "from skimage import io as skio\n",
        "from skimage import filters\n",
        "from scipy import misc\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLoyIQYQPA35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfoeA7CrlSrJ"
      },
      "source": [
        "# Global variables and utilities\n",
        "\n",
        "# Variables\n",
        "base_path = '/content/drive/MyDrive/dataset'\n",
        "filenames = sorted(os.listdir(base_path))\n",
        "\n",
        "\n",
        "# utilities\n",
        "def make_folder(path):\n",
        "  \"\"\" Check if a folder exists, if it doesn't exist create one in the given path\n",
        "  Args:\n",
        "    path [str]: path where the folder needs to be crerated\n",
        "  \"\"\"\n",
        "\n",
        "  if not os.path.exists(os.path.join(path)):\n",
        "    print(f'[INFO] Creating new folder {path}...')\n",
        "    os.makedirs(os.path.join(path))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BL0Vy6nXp8p"
      },
      "source": [
        "<a name=\"data-preparation\"></a>\n",
        "## 1 Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGvAae0XYCHx"
      },
      "source": [
        "### 1.1 Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZRAurloN2iN"
      },
      "source": [
        "'''\r\n",
        "Gray images are resized to be of equal size and proportion, so as to reduce computation time.\r\n",
        "\r\n",
        "'''\r\n",
        "def remove_background(image_rgb):\r\n",
        "  height, width = image_rgb.shape[:2]\r\n",
        "\r\n",
        "  rectangle = (10,10,width-30,height-30)\r\n",
        "  # Create initial mask\r\n",
        "  mask = np.zeros(image_rgb.shape[:2], np.uint8)\r\n",
        "\r\n",
        "  # Create temporary arrays used by grabCut\r\n",
        "  bgdModel = np.zeros((1, 65), np.float64)\r\n",
        "  fgdModel = np.zeros((1, 65), np.float64)\r\n",
        "\r\n",
        "  # Run grabCut\r\n",
        "  cv2.grabCut(image_rgb, # Our image\r\n",
        "              mask, # The Mask\r\n",
        "              rectangle, # Our rectangle\r\n",
        "              bgdModel, # Temporary array for background\r\n",
        "              fgdModel, # Temporary array for background\r\n",
        "              5, # Number of iterations\r\n",
        "              cv2.GC_INIT_WITH_RECT) # Initiative using our rectangle\r\n",
        "\r\n",
        "  # Create mask where sure and likely backgrounds set to 0, otherwise 1\r\n",
        "  mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\r\n",
        "\r\n",
        "  # Multiply image with new mask to subtract background\r\n",
        "  image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]\r\n",
        "\r\n",
        "  return image_rgb_nobg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etG7umlWRzF"
      },
      "source": [
        "# Image processing code\n",
        "def resize_image(image):\n",
        "  \"\"\"\n",
        "  Resizes the image to a 100 by 100.\n",
        "  Args:\n",
        "    image (numpy array): Array of image pixels.\n",
        "\n",
        "  Returns:\n",
        "    (numpy array): Resized image\n",
        "  \"\"\"\n",
        "  image = resize(image, (100,100))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDcpWyW0dCxm"
      },
      "source": [
        "def threshold(image, value):\r\n",
        "  image = image > value\r\n",
        "  return image\r\n",
        "\r\n",
        "def get_threshold_image(image):\r\n",
        "\r\n",
        "  #converts image np array to grey scale\r\n",
        "  r, g, b = image[:,:,0], image[:,:,1], image[:,:,2]\r\n",
        "  gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\r\n",
        "  i = np.dot(image[...,:3], [0.299, 0.587, 0.114])\r\n",
        "\r\n",
        "  im_th = threshold(i, 0.2)\r\n",
        "  \r\n",
        "  new_image = util.img_as_ubyte(im_th, force_copy=False)\r\n",
        "\r\n",
        "  filled_image = ndimage.binary_fill_holes(new_image)\r\n",
        "  \r\n",
        "  return new_image\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0rZLUvma6fH"
      },
      "source": [
        "def process_images(image):\n",
        "  # image_bgr = cv2.imread(image_path)\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image_no_bgr = remove_background(image_rgb)\n",
        "  resized_image = resize_image(image_no_bgr)\n",
        "  image_thresh = get_threshold_image(resized_image)\n",
        "\n",
        "  return image_thresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJjAvQSqYP_z"
      },
      "source": [
        "### 1.2 Load images and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU6dythUeUeb"
      },
      "source": [
        "In order to use the images in training the machine learning model we had to load and label the images. As these images were not labelled from source we had to devise how to link a label to a particular image. Two arrays were created; One to hold the images and the other to hold the label of these images. \n",
        "The images stored into the array undergo preprocessing before they are stored in the array, so that images in the array are ready for subsequent steps of the pipeline.\n",
        "\n",
        "so when an image is loaded, the label is also loaded. This ensures that the label of image[i] is present at label[i] making it easier to work with the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lGWNkjLkRz8v"
      },
      "source": [
        "def get_image_path(path, foldername=\"\"):\n",
        "    \"\"\"\n",
        "    The function reads images from a location, specified by the path returning an array containing the images in that\n",
        "    location\n",
        "    :param path: the location of the data.\n",
        "    :param foldername: this by default is an empty string but can be used if the path contains multiple folders you\n",
        "    you can then parse using a for loop and changing the value of the parameter at every iteration\n",
        "    :return: an array of images in the path/filename directory\n",
        "    \"\"\"\n",
        "    images = sorted([os.path.join(path, foldername, file) for file in os.listdir(os.path.join(path, foldername))])\n",
        "    return images\n",
        "\n",
        "\n",
        "def get_label(file_name):\n",
        "  label = file_name.replace(\"___\", \" \")\n",
        "  label = label.replace(\"_\", \" \")\n",
        "  return label\n",
        "\n",
        "\n",
        "def load_dataset(path, filenames):\n",
        "    \"\"\"\n",
        "    load_dataset loads all of the images in the plantvillage dataset storing them in a dictionary with keys representing\n",
        "    the foldernames and the values are arrays containing the images in the folder\n",
        "    :param path: the location of the data\n",
        "    :param filenames: a list of the folders in the path. This can be obtained by running sorted(os.listdir(path))\n",
        "    :return: a dictionary with keys identifying the crops and diseased state(healthy or the type of diseases)and values\n",
        "    which are arrays containing the images in that folder\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    for i in filenames:\n",
        "        # Ignore ipynb checkpoints and any other plant except apple\n",
        "        if i == '.ipynb_checkpoints' or \"Apple\" not in i :\n",
        "          continue\n",
        "        img_arr = get_image_path(path, i)\n",
        "        for img in img_arr:\n",
        "          img = plt.imread(img)\n",
        "          img = process_images(img)\n",
        "          images.append(img)\n",
        "          labels.append(get_label(i))\n",
        "    img_arr = np.array(images)\n",
        "    label_arr = np.array(labels)\n",
        "          \n",
        "    return images,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE7wa7oJYcOI"
      },
      "source": [
        "<a name=\"feature-extraction\"></a>\n",
        "## 2 Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDGPIlofkb62"
      },
      "source": [
        "For extracting features from the images, we made use of the [hog function](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) present in the skimage feature module.\n",
        "\n",
        "[Histogram of oriented gradients(HOG) ](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) is a feature descriptor that is often used to extract features from image data widely used in computer vision tasks for object detection. The HOG feature descriptor counts the occurrences of gradient orientation in localized portions of an image.\n",
        "\n",
        "The extract feature function takes loops through all of the train, test and validation datasets extracting the features from them and stores these extracted features for future use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY-nmxWr5pCf"
      },
      "source": [
        "def hog_feature(image):\r\n",
        "  \"\"\"\r\n",
        "  Extract HOG feature descriptors from the image\r\n",
        "\r\n",
        "  Args: \r\n",
        "    image (numpy array): Array of image pixels.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    (numpy array): Feature descriptors\r\n",
        "  \"\"\"\r\n",
        "  return hog(image)\r\n",
        "\r\n",
        "def extract_feature():\r\n",
        "  \"\"\"\r\n",
        "  Load images and extract HOG feature desscriptors.\r\n",
        "  \"\"\"\r\n",
        "  X_train = np.load(os.path.join(base_path, \"augment/imageaugment_input.npy\"))\r\n",
        "  print(\"===TRAIN DATA===\")\r\n",
        "  print(len(X_train))\r\n",
        "  print(X_train.shape)\r\n",
        "\r\n",
        "  X_test = np.load(os.path.join(base_path, \"test/imagetest_input.npy\"))\r\n",
        "  print(\"===TEST DATA===\")\r\n",
        "  print(len(X_test))\r\n",
        "  print(X_test.shape)\r\n",
        "\r\n",
        "  print(\"Extracting HOG features...\")\r\n",
        "  RF_train = np.zeros([len(X_train), 8100])\r\n",
        "  for i in range(len(X_train)):\r\n",
        "    RF_train[i] = hog_feature(X_train[i])\r\n",
        "  print(\"FEATURE DESCRIPTORS\")\r\n",
        "  print(len(RF_train))\r\n",
        "\r\n",
        "  RF_test = np.zeros([len(X_test), 8100])\r\n",
        "  for i in range(len(X_test)):\r\n",
        "    RF_test[i] = hog_feature(X_test[i])\r\n",
        "  print(len(RF_test))\r\n",
        "\r\n",
        "  # Save data\r\n",
        "  make_folder(os.path.join(base_path, \"processed\"))\r\n",
        "  np.save(os.path.join(base_path, \"processed/imagetrainHOG_input.npy\"), RF_train)\r\n",
        "  np.save(os.path.join(base_path, \"processed/imagetestHOG_input.npy\"), RF_test)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EquAwaflYjF7"
      },
      "source": [
        "<a name=\"data-augmentation\"></a>\n",
        "## 3 Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4AK3ejNnx5N"
      },
      "source": [
        "Data Augmentation is generally carried out in order to increase the amount of data by adding slightly modified copies of already existing data or newly synthetic data from existing data. This process also acts as a regularizer and helps reduce overfitting when training models.\n",
        "\n",
        "For our project, we augmented the training data by performing geometric transformations on each sample. These includes: \n",
        "\n",
        "\n",
        "\n",
        "*   rotating the image at a random angle\n",
        "*   flipping the image horizontally\n",
        "*   flipping the image vertically\n",
        "*   gama correction\n",
        "*   intensity correction\n",
        "\n",
        "These transformation especially the rotations and flipping of the images also serve to provide a wide range of images type as images upload to the deployed images might be rotated or flipped allowing the model to become used to those kind of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbLY4uvvJCuq"
      },
      "source": [
        "# Data Augmentation\n",
        "def random_rotation(img):\n",
        "  \"\"\"\n",
        "  Randomly rotate the image.\n",
        "  The function picks a random degree of rotation between 25% on the \n",
        "  left and 25% on the right\n",
        "\n",
        "  Args:\n",
        "    img (numpy array): Array of image pixel to rotate\n",
        "\n",
        "  Returns:\n",
        "    (numpy array): Rotated image.\n",
        "  \"\"\"\n",
        "  random_deg = random.uniform(-25, 25)\n",
        "  return rotate(img, random_deg, preserve_range=True).astype(np.uint8)\n",
        "\n",
        "def horizontal_flip(img):\n",
        "  \"\"\"\n",
        "  Flip the image horizontally\n",
        "  Args:\n",
        "    img (numpy array): Array of image pixels.\n",
        "\n",
        "  Returns:\n",
        "    (numpy array): Flipped image.\n",
        "  \"\"\"\n",
        "  return np.fliplr(img)\n",
        "\n",
        "def vertical_flip(img):\n",
        "  \"\"\"\n",
        "  Flip the image vertically\n",
        "  Args:\n",
        "    img (numpy array): Array of image pixels\n",
        "  Returns:\n",
        "    (numpy array): Flipped image.\n",
        "  \"\"\"\n",
        "  return np.flipud(img)\n",
        "\n",
        "def gama(img):\n",
        "  \"\"\"\n",
        "  Perform gamma correction of the image\n",
        "\n",
        "  Args:\n",
        "    img (numpy arrayy): Array of image pixes\n",
        "\n",
        "  Returns:\n",
        "    (numpy array): Enhanced image.\n",
        "  \"\"\"\n",
        "  return exposure.adjust_gamma(img, gamma=0.4, gain=0.9)\n",
        "\n",
        "\n",
        "def intensity(img):\n",
        "    \"\"\" \n",
        "    Change the intensity of the image.\n",
        "    Args:\n",
        "        img (numpy array): Array of image pixels.\n",
        "    Returns:\n",
        "        (numpy array): Enhanced image.\n",
        "    \"\"\"\n",
        "    v_min, v_max = np.percentile(img, (0.2, 99.8))\n",
        "    if np.abs(v_max - v_min) < 1e-3:\n",
        "        v_max += 1e-3\n",
        "    return exposure.rescale_intensity(img, in_range=(v_min, v_max))\n",
        "\n",
        "def augment_image(img, label):\n",
        "  \"\"\"\n",
        "  Performs image augmentation using rotation, intensity scaling,\n",
        "  flip and gamma correction.\n",
        "  Args:\n",
        "    img (numpy array): array of image pixels.\n",
        "    label (str): Label of the image.\n",
        "  Returns:\n",
        "    (numpy array): Augmented images.\n",
        "    (numpy array): Array of labels corresponding to the images.\n",
        "  \"\"\"\n",
        "  temp = [horizontal_flip(img), vertical_flip(img), random_rotation(img), gama(img), intensity(img)]\n",
        "  label = [label, label, label, label, label]\n",
        "  return temp, label\n",
        "\n",
        "\n",
        "def augment_data():\n",
        "  \"\"\"\n",
        "  Load the train data and augment loaded data.\n",
        "  \"\"\"\n",
        "  X_train = np.load(os.path.join(base_path, \"intermediate/imagetrain_input.npy\"))\n",
        "  y_train = np.load(os.path.join(base_path, \"intermediate/labeltrain_input.npy\"))\n",
        "  print(\"DATA TO BE AUGMENTED\")\n",
        "  print(len(X_train))\n",
        "  print(len(y_train))\n",
        "\n",
        "  as_count = br_count = ar_count = 0\n",
        "  transformed_image = []\n",
        "  labels = []\n",
        "\n",
        "  for i, name in enumerate(y_train):\n",
        "    if name == 'Apple healthy':\n",
        "      x, y = augment_image(X_train[i], name)\n",
        "      transformed_image.extend(x)\n",
        "      labels.extend(y)\n",
        "    elif name == 'Apple Apple scab':\n",
        "      x, y = augment_image(X_train[i], name)\n",
        "      transformed_image.extend(x)\n",
        "      labels.extend(y)\n",
        "      as_count += 1\n",
        "    elif name == 'Apple Black rot':\n",
        "      x, y = augment_image(X_train[i], name)\n",
        "      transformed_image.extend(x)\n",
        "      labels.extend(y)\n",
        "      br_count += 1    \n",
        "    elif name == 'Apple Cedar apple rust':\n",
        "      x, y = augment_image(X_train[i], name)\n",
        "      transformed_image.extend(x)\n",
        "      labels.extend(y)\n",
        "      ar_count += 1\n",
        "\n",
        "  transformed_image = np.array(transformed_image)\n",
        "  labels = np.array(labels)\n",
        "  print(\"AUGMENTED DATA\")\n",
        "  print(len(transformed_image))\n",
        "  print(len(labels))\n",
        "\n",
        "  # Concatenate with initial image array\n",
        "  X_train = np.concatenate((X_train, transformed_image), axis=0)\n",
        "  y_train = np.concatenate((y_train, labels), axis=0)\n",
        "  print(\"TOTAL MODEL INPUT DATA\")\n",
        "  print(len(X_train))\n",
        "  print(len(y_train))\n",
        "\n",
        "  # Save data\n",
        "  make_folder(os.path.join(base_path, \"augment\"))\n",
        "  np.save(os.path.join(base_path, \"augment/imageaugment_input.npy\"), X_train)\n",
        "  np.save(os.path.join(base_path, \"augment/labelaugment_input.npy\"), y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsppNKRfYtCo"
      },
      "source": [
        "<a name=\"supervised-learning\"></a>\n",
        "## 4 Supervised Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKkLpATY7Be"
      },
      "source": [
        "### 4.1 Classification Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8iHqM9Ejm2l"
      },
      "source": [
        "# SVM model\n",
        "def run_svm():\n",
        "  \"\"\"\n",
        "  Load the data. Train SVM model using linear kernel. Print accuracy on test dat.\n",
        "  \"\"\"\n",
        "  X_train = np.load(os.path.join(base_path, \"processed/imagetrainHOG_input.npy\"))\n",
        "  y_train = np.load(os.path.join(base_path, \"augment/labelaugment_input.npy\"))\n",
        "\n",
        "  print(\"=== TRAIN DATA ===\")\n",
        "  print(len(X_train))\n",
        "  print(len(y_train))\n",
        "\n",
        "  X_test = np.load(os.path.join(base_path, \"processed/imagetestHOG_input.npy\"))\n",
        "  y_test = np.load(os.path.join(base_path, \"test/labelstest_input.npy\"))\n",
        "\n",
        "  print(\"=== TEST DATA ===\")\n",
        "  print(len(X_test))\n",
        "  print(len(y_test))\n",
        "\n",
        "  svm_model = LinearSVC(dual=False)\n",
        "  svm_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "  print(f\"SVM accuracy score {svm_model.score(X_test, y_test)}\")\n",
        "   \n",
        "  make_folder(os.path.join(base_path, \"results/models\"))\n",
        "  joblib.dump(svm_model, os.path.join(base_path, \"results/models/SVM_MODEL.sav\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBChzQfcjznd"
      },
      "source": [
        "# Random Forest model\n",
        "def run_random_forest():\n",
        "  \"\"\"\n",
        "  Load the data. Train the model and print out the accuracy on test data\n",
        "  \"\"\"\n",
        "\n",
        "  X_train = np.load(os.path.join(base_path, \"processed/imagetrainHOG_input.npy\"))\n",
        "  y_train = np.load(os.path.join(base_path, \"augment/labelaugment_input.npy\"))\n",
        "  print(\"=== TRAIN DATA ===\")\n",
        "  print(len(X_train))\n",
        "  print(len(y_train))\n",
        "\n",
        "  X_test = np.load(os.path.join(base_path, \"processed/imagetestHOG_input.npy\"))\n",
        "  y_test = np.load(os.path.join(base_path, \"test/labelstest_input.npy\"))\n",
        "\n",
        "  print(\"=== TEST DATA ===\")\n",
        "  print(len(X_test))\n",
        "  print(len(y_test))\n",
        "\n",
        "  # Classifier\n",
        "  Random_classifier = RandomForestClassifier(n_estimators=500, max_depth=35,\n",
        "                                               n_jobs=-1, warm_start=True,\n",
        "                                               oob_score=True,\n",
        "                                               max_features='sqrt')\n",
        "  Random_classifier.fit(X_train, y_train)\n",
        "  print(f\"Random Forest accuracy score {Random_classifier.score(X_test, y_test)}\")\n",
        "\n",
        "  make_folder(os.path.join(base_path, \"results/models\"))\n",
        "  joblib.dump(Random_classifier, os.path.join(base_path, \"results/models/RANDOM_FOREST_MODEL.sav\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRkkU50of2-Z"
      },
      "source": [
        "# KNeighbors classifier\n",
        "def run_kneighbors():\n",
        "  \"\"\"\n",
        "  Load the data. Train the model and print out the accuracy on test data\n",
        "  \"\"\"\n",
        "\n",
        "  X_train = np.load(os.path.join(base_path, \"processed/imagetrainHOG_input.npy\"))\n",
        "  y_train = np.load(os.path.join(base_path, \"augment/labelaugment_input.npy\"))\n",
        "  print(\"=== TRAIN DATA ===\")\n",
        "  print(len(X_train))\n",
        "  print(len(y_train))\n",
        "\n",
        "  X_test = np.load(os.path.join(base_path, \"processed/imagetestHOG_input.npy\"))\n",
        "  y_test = np.load(os.path.join(base_path, \"test/labelstest_input.npy\"))\n",
        "\n",
        "  print(\"=== TEST DATA ===\")\n",
        "  print(len(X_test))\n",
        "  print(len(y_test))\n",
        "\n",
        "  # Classifier\n",
        "  clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(f\"K Neighbors accuracy score {clf.score(X_test, y_test)}\")\n",
        "\n",
        "  make_folder(os.path.join(base_path, \"results/models\"))\n",
        "  joblib.dump(clf, os.path.join(base_path, \"results/models/KNeighbors.sav\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BO6oFyqYxlC"
      },
      "source": [
        "### 4.2 Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUaTHdwxkBiU"
      },
      "source": [
        "# Predict single image\n",
        "def predict(img):\n",
        "  \"\"\"\n",
        "  Predict the classification of a single image\n",
        "  Args:\n",
        "    img(numpy array)\n",
        "  Returns:\n",
        "    (string) label for images\n",
        "  \"\"\"\n",
        "  # take the image through preprocessing\n",
        "  img = plt.imread(img)\n",
        "  img = process_images(img)\n",
        "  img = np.array(img)\n",
        "  img = hog_feature(img)\n",
        "  \n",
        "  img = img.reshape(1, -1)\n",
        "\n",
        "  # load the models\n",
        "  svm_model = joblib.load(os.path.join(base_path, \"results/models/SVM_MODEL.sav\"))\n",
        "  rfc_model = joblib.load(os.path.join(base_path, \"results/models/RANDOM_FOREST_MODEL.sav\"))\n",
        "  knn_model = joblib.load(os.path.join(base_path, \"results/models/KNeighbors.sav\"))\n",
        "\n",
        "  # Predict\n",
        "  svm_label = svm_model.predict(img)\n",
        "  rfc_label = rfc_model.predict(img)\n",
        "  knn_label = knn_model.predict(img)\n",
        "\n",
        "  print(f\"SVM Classifier predicts {svm_label}\")\n",
        "  print(f\"Random Forest Classifier predicts {rfc_label}\")\n",
        "  print(f\"KNN predicts {knn_label}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WrEKEPWFRz8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c933f11a-8536-4759-a251-ca2d4ca3f0e8"
      },
      "source": [
        "#Load the Images\n",
        "images, labels = load_dataset(base_path, filenames)\n",
        "print('=== TOTAL DATA ===')\n",
        "print(len(labels))\n",
        "print(len(images))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== TOTAL DATA ===\n",
            "2336\n",
            "2336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGuAn_wNZKan"
      },
      "source": [
        "<a name=\"train-validation-test-split\"></a>\n",
        "## 5 Train Validation Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4kwmmnyBI6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3721ddf3-c72d-49e7-eedd-d7e3f0e9c19a"
      },
      "source": [
        "# Shuffle data\n",
        "images, labels = shuffle(images, labels, random_state=42)\n",
        "print(np.unique(labels))\n",
        "\n",
        "# Split train set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state= 42)\n",
        "\n",
        "print('=== TRAIN TEST SPLIT ===')\n",
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple Apple scab' 'Apple Black rot' 'Apple Cedar apple rust'\n",
            " 'Apple healthy']\n",
            "=== TRAIN TEST SPLIT ===\n",
            "1868\n",
            "468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgkUk0pSFvxJ"
      },
      "source": [
        "# Save data\n",
        "base_path = '/content/drive/MyDrive/dataset'\n",
        "make_folder(os.path.join(base_path, \"test\"))\n",
        "make_folder(os.path.join(base_path, \"intermediate\"))\n",
        "np.save(os.path.join(base_path, \"test/imagetest_input.npy\"), X_test)\n",
        "np.save(os.path.join(base_path, \"test/labelstest_input.npy\"), y_test)\n",
        "np.save(os.path.join(base_path, \"intermediate/imagetrain_input.npy\"), X_train)\n",
        "np.save(os.path.join(base_path, \"intermediate/labeltrain_input.npy\"), y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQtWZwrDZXbU"
      },
      "source": [
        "<a name=\"results\"></a>\n",
        "## 6 Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgzmdi7vqWtH"
      },
      "source": [
        "### 6.1 Baseline performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1J-dQHaqfsh"
      },
      "source": [
        "![Random_forest2.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo-dG3sov5A1",
        "outputId": "2178aa3b-7541-43c1-d104-a6b4a559e9a3"
      },
      "source": [
        "augment_data()\n",
        "extract_feature()\n",
        "run_svm()\n",
        "run_random_forest()\n",
        "run_kneighbors()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA TO BE AUGMENTED\n",
            "1868\n",
            "1868\n",
            "AUGMENTED DATA\n",
            "9340\n",
            "9340\n",
            "TOTAL MODEL INPUT DATA\n",
            "11208\n",
            "11208\n",
            "===TRAIN DATA===\n",
            "11208\n",
            "(11208, 100, 100)\n",
            "===TEST DATA===\n",
            "468\n",
            "(468, 100, 100)\n",
            "Extracting HOG features...\n",
            "FEATURE DESCRIPTORS\n",
            "11208\n",
            "468\n",
            "=== TRAIN DATA ===\n",
            "11208\n",
            "11208\n",
            "=== TEST DATA ===\n",
            "468\n",
            "468\n",
            "SVM accuracy score 0.6175213675213675\n",
            "=== TRAIN DATA ===\n",
            "11208\n",
            "11208\n",
            "=== TEST DATA ===\n",
            "468\n",
            "468\n",
            "Random Forest accuracy score 0.7200854700854701\n",
            "=== TRAIN DATA ===\n",
            "11208\n",
            "11208\n",
            "=== TEST DATA ===\n",
            "468\n",
            "468\n",
            "K Neighbors accuracy score 0.6111111111111112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTl0lVICqpyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c22fdb4-43f7-4696-b666-739ec6cb5eb9"
      },
      "source": [
        "predict(\"/content/drive/MyDrive/dataset/Apple___Apple_scab/00075aa8-d81a-4184-8541-b692b78d398a___FREC_Scab 3335.JPG\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Classifier predicts ['Apple Apple scab']\n",
            "Random Forest Classifier predicts ['Apple Apple scab']\n",
            "KNN predicts ['Apple Apple scab']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7savt6AZj2R"
      },
      "source": [
        "<a name=\"exploration-for-future-works\"></a>\n",
        "## 7 Exploration for future works\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFMsUjfoZrJn"
      },
      "source": [
        "<a name=\"conclusions\"></a>\n",
        "## 8 Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-efmEKAYZt_H"
      },
      "source": [
        "<a name=\"acknowledgements\"></a>\n",
        "## 9 Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YJCy8GxZ5Jr"
      },
      "source": [
        "<a name=\"references\"></a>\n",
        "## 10 References"
      ]
    }
  ]
}